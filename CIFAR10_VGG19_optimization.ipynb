{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 30919,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "papermill": {
      "default_parameters": {},
      "duration": 5541.892577,
      "end_time": "2025-03-20T09:07:17.065799",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2025-03-20T07:34:55.173222",
      "version": "2.6.0"
    },
    "colab": {
      "name": "CIFAR10-VGG19-optimization",
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Auniik/Auniik/blob/main/CIFAR10_VGG19_optimization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchinfo tqdm"
      ],
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "execution": {
          "iopub.status.busy": "2025-03-22T13:54:36.214755Z",
          "iopub.execute_input": "2025-03-22T13:54:36.215047Z",
          "iopub.status.idle": "2025-03-22T13:54:39.521788Z",
          "shell.execute_reply.started": "2025-03-22T13:54:36.215015Z",
          "shell.execute_reply": "2025-03-22T13:54:39.520772Z"
        },
        "papermill": {
          "duration": 4.15014,
          "end_time": "2025-03-20T07:35:01.921488",
          "exception": false,
          "start_time": "2025-03-20T07:34:57.771348",
          "status": "completed"
        },
        "tags": [],
        "trusted": true,
        "id": "76d5d474"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initial Setup"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.008076,
          "end_time": "2025-03-20T07:35:01.938518",
          "exception": false,
          "start_time": "2025-03-20T07:35:01.930442",
          "status": "completed"
        },
        "tags": [],
        "id": "bc6ed47d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import gc\n",
        "import time\n",
        "from pathlib import Path\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.onnx\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from torchinfo import summary\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-03-22T13:54:39.522854Z",
          "iopub.execute_input": "2025-03-22T13:54:39.523214Z",
          "iopub.status.idle": "2025-03-22T13:54:42.131349Z",
          "shell.execute_reply.started": "2025-03-22T13:54:39.523179Z",
          "shell.execute_reply": "2025-03-22T13:54:42.130271Z"
        },
        "papermill": {
          "duration": 6.944355,
          "end_time": "2025-03-20T07:35:08.891357",
          "exception": false,
          "start_time": "2025-03-20T07:35:01.947002",
          "status": "completed"
        },
        "tags": [],
        "trusted": true,
        "id": "64119cd4"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "np.random.seed(42)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-03-22T13:54:42.133523Z",
          "iopub.execute_input": "2025-03-22T13:54:42.134015Z",
          "iopub.status.idle": "2025-03-22T13:54:42.14061Z",
          "shell.execute_reply.started": "2025-03-22T13:54:42.133973Z",
          "shell.execute_reply": "2025-03-22T13:54:42.139798Z"
        },
        "papermill": {
          "duration": 0.018096,
          "end_time": "2025-03-20T07:35:08.91817",
          "exception": false,
          "start_time": "2025-03-20T07:35:08.900074",
          "status": "completed"
        },
        "tags": [],
        "trusted": true,
        "id": "0cf8ebef"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Loader Function\n",
        "\n",
        "This function prepares data loaders for the CIFAR-10 dataset, tailored for use with models like VGG19 that expect RGB images of size 224x224. It includes transformations to adjust the CIFAR-10 grayscale images and create appropriate subsets for training and validation.t."
      ],
      "metadata": {
        "papermill": {
          "duration": 0.008257,
          "end_time": "2025-03-20T07:35:08.934862",
          "exception": false,
          "start_time": "2025-03-20T07:35:08.926605",
          "status": "completed"
        },
        "tags": [],
        "id": "fe8975ed"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data_loaders(num_samples=5000, batch_size=64):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.4914, 0.4822, 0.4465],\n",
        "                             std=[0.2023, 0.1994, 0.2010])\n",
        "    ])\n",
        "\n",
        "    train_dataset = torchvision.datasets.CIFAR10(\n",
        "        root='./data', train=True, download=True, transform=transform\n",
        "    )\n",
        "    test_dataset = torchvision.datasets.CIFAR10(\n",
        "        root='./data', train=False, download=True, transform=transform\n",
        "    )\n",
        "\n",
        "    train_subset = Subset(train_dataset, range(num_samples))\n",
        "    val_subset = Subset(test_dataset, range(num_samples // 5))\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_subset, batch_size=batch_size, shuffle=True,\n",
        "        num_workers=2, pin_memory=True\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        val_subset, batch_size=batch_size, shuffle=False,\n",
        "        num_workers=2, pin_memory=True\n",
        "    )\n",
        "\n",
        "    return train_loader, val_loader"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-03-22T13:54:42.142179Z",
          "iopub.execute_input": "2025-03-22T13:54:42.142466Z",
          "iopub.status.idle": "2025-03-22T13:54:42.163183Z",
          "shell.execute_reply.started": "2025-03-22T13:54:42.14244Z",
          "shell.execute_reply": "2025-03-22T13:54:42.162339Z"
        },
        "papermill": {
          "duration": 0.015693,
          "end_time": "2025-03-20T07:35:08.958824",
          "exception": false,
          "start_time": "2025-03-20T07:35:08.943131",
          "status": "completed"
        },
        "tags": [],
        "trusted": true,
        "id": "45e9d6a7"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### VGG19 Model Preparation Function\n",
        "\n",
        "This function loads a pre-trained VGG19 model from the ImageNet dataset and modifies its final classification layer to adapt it for a different dataset.\n"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.00796,
          "end_time": "2025-03-20T07:35:08.975153",
          "exception": false,
          "start_time": "2025-03-20T07:35:08.967193",
          "status": "completed"
        },
        "tags": [],
        "id": "6d5c9c2c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model(classes = 10):\n",
        "    model = torchvision.models.vgg19(weights='IMAGENET1K_V1')\n",
        "    model.classifier[6] = nn.Linear(4096, classes)\n",
        "    return model"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-03-22T13:54:42.164211Z",
          "iopub.execute_input": "2025-03-22T13:54:42.164496Z",
          "iopub.status.idle": "2025-03-22T13:54:42.183702Z",
          "shell.execute_reply.started": "2025-03-22T13:54:42.16447Z",
          "shell.execute_reply": "2025-03-22T13:54:42.18287Z"
        },
        "papermill": {
          "duration": 0.013651,
          "end_time": "2025-03-20T07:35:08.996852",
          "exception": false,
          "start_time": "2025-03-20T07:35:08.983201",
          "status": "completed"
        },
        "tags": [],
        "trusted": true,
        "id": "be412c58"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### FLOPs Estimation Function\n",
        "**Returns:**\n",
        "- `total_flops` (int): The estimated total number of floating-point operations for both the forward and backward passes.\n"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.007842,
          "end_time": "2025-03-20T07:35:09.013012",
          "exception": false,
          "start_time": "2025-03-20T07:35:09.00517",
          "status": "completed"
        },
        "tags": [],
        "id": "ae4e19f7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def count_flops(model, input_size=(1, 3, 224, 224)):\n",
        "    \"\"\"Estimate FLOPs for the model\"\"\"\n",
        "    total_flops = 0\n",
        "    for param in model.parameters():\n",
        "        if len(param.shape) >= 2:\n",
        "            total_flops += np.prod(param.shape)\n",
        "    return total_flops * 2\n",
        "\n",
        "def get_model_file_size(model):\n",
        "    torch.save(model.state_dict(), \"temp.p\")\n",
        "    size = os.path.getsize(\"temp.p\") / (1024 * 1024)\n",
        "    os.remove(\"temp.p\")\n",
        "    return size"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-03-22T13:54:42.184578Z",
          "iopub.execute_input": "2025-03-22T13:54:42.18484Z",
          "iopub.status.idle": "2025-03-22T13:54:42.199472Z",
          "shell.execute_reply.started": "2025-03-22T13:54:42.184819Z",
          "shell.execute_reply": "2025-03-22T13:54:42.198702Z"
        },
        "papermill": {
          "duration": 0.014059,
          "end_time": "2025-03-20T07:35:09.035115",
          "exception": false,
          "start_time": "2025-03-20T07:35:09.021056",
          "status": "completed"
        },
        "tags": [],
        "trusted": true,
        "id": "ba430dc3"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Function for One Epoch\n",
        "\n",
        "This function performs one epoch of training for a given model using the provided data loader, loss function, and optimizer. It includes essential features such as gradient clipping and NaN loss handling to ensure stable and effective training.\n",
        "\n",
        "**Key Features:**\n",
        "- **Model Training Mode:** Puts the model in training mode to enable features like dropout and batch normalization.\n",
        "- **Loss Computation:** Uses the specified criterion (loss function) to calculate the training loss.\n",
        "- **Gradient Clipping:** Prevents exploding gradients by clipping them to a maximum norm (`max_grad_norm`).\n",
        "- **NaN Loss Handling:** Detects and skips batches with NaN loss values to maintain training stability.\n",
        "- **Real-Time Progress Monitoring:** Uses `tqdm` to display real-time progress, including current loss and accuracy.\n",
        "- **Accuracy Calculation:** Tracks the number of correct predictions to compute accuracy for the epoch.\n"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.007794,
          "end_time": "2025-03-20T07:35:09.050957",
          "exception": false,
          "start_time": "2025-03-20T07:35:09.043163",
          "status": "completed"
        },
        "tags": [],
        "id": "7b63e587"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(model, train_loader, criterion, optimizer, device, max_grad_norm=1.0):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    pbar = tqdm(train_loader, desc='Training')\n",
        "    for inputs, targets in pbar:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "\n",
        "        # Check for NaN loss\n",
        "        if torch.isnan(loss):\n",
        "            print(\"Warning: NaN loss detected. Skipping batch.\")\n",
        "            continue\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        # Gradient clipping\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "        pbar.set_postfix({\n",
        "            'loss': f'{running_loss/total:.3f}',\n",
        "            'acc': f'{100.*correct/total:.1f}%'\n",
        "        })\n",
        "\n",
        "    return running_loss / len(train_loader), 100.*correct/total"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-03-22T13:54:42.200251Z",
          "iopub.execute_input": "2025-03-22T13:54:42.200487Z",
          "iopub.status.idle": "2025-03-22T13:54:42.215651Z",
          "shell.execute_reply.started": "2025-03-22T13:54:42.200468Z",
          "shell.execute_reply": "2025-03-22T13:54:42.214841Z"
        },
        "papermill": {
          "duration": 0.015492,
          "end_time": "2025-03-20T07:35:09.074427",
          "exception": false,
          "start_time": "2025-03-20T07:35:09.058935",
          "status": "completed"
        },
        "tags": [],
        "trusted": true,
        "id": "ce659b24"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Evaluation Function\n",
        "\n",
        "This function evaluates a trained model on a validation dataset, providing key performance metrics such as loss, accuracy.\n",
        "\n",
        "**Key Features:**\n",
        "- **Evaluation Mode:** Sets the model to evaluation mode, disabling features like dropout and batch normalization updates.\n",
        "- **No Gradient Calculation:** Uses `torch.no_grad()` for efficient evaluation by disabling gradient computations.\n",
        "- **Top-1 and Top-5 Accuracy:** Computes both standard accuracy (Top-1) and Top-5 accuracy, useful for multi-class classification tasks."
      ],
      "metadata": {
        "papermill": {
          "duration": 0.007919,
          "end_time": "2025-03-20T07:35:09.090437",
          "exception": false,
          "start_time": "2025-03-20T07:35:09.082518",
          "status": "completed"
        },
        "tags": [],
        "id": "c3d7b895"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, val_loader, criterion, device):\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    correct_top5 = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in val_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            # Top-1 accuracy\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "            # Top-5 accuracy\n",
        "            _, pred5 = outputs.topk(5, 1, True, True)\n",
        "            correct_top5 += pred5.eq(targets.view(-1, 1).expand_as(pred5)).sum().item()\n",
        "\n",
        "    return {\n",
        "        'loss': running_loss / len(val_loader),\n",
        "        'acc': 100. * correct / total,\n",
        "        'top1_acc': 100. * correct / total,\n",
        "        'top5_acc': 100. * correct_top5 / total\n",
        "    }"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-03-22T13:54:42.216403Z",
          "iopub.execute_input": "2025-03-22T13:54:42.216662Z",
          "iopub.status.idle": "2025-03-22T13:54:42.236467Z",
          "shell.execute_reply.started": "2025-03-22T13:54:42.216635Z",
          "shell.execute_reply": "2025-03-22T13:54:42.235658Z"
        },
        "papermill": {
          "duration": 0.014749,
          "end_time": "2025-03-20T07:35:09.113166",
          "exception": false,
          "start_time": "2025-03-20T07:35:09.098417",
          "status": "completed"
        },
        "tags": [],
        "trusted": true,
        "id": "495108ae"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "class EarlyStopping:\n",
        "    def __init__(self, patience=5, min_delta=0):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.best_model_state = None\n",
        "\n",
        "    def __call__(self, val_loss, model):\n",
        "        score = -val_loss\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.best_model = model\n",
        "        elif score < self.best_score + self.min_delta:\n",
        "            self.counter += 1\n",
        "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.best_model_state = model.state_dict()\n",
        "            self.counter = 0"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-03-22T13:54:42.237428Z",
          "iopub.execute_input": "2025-03-22T13:54:42.237717Z",
          "iopub.status.idle": "2025-03-22T13:54:42.255809Z",
          "shell.execute_reply.started": "2025-03-22T13:54:42.237689Z",
          "shell.execute_reply": "2025-03-22T13:54:42.254944Z"
        },
        "papermill": {
          "duration": 0.014605,
          "end_time": "2025-03-20T07:35:09.13601",
          "exception": false,
          "start_time": "2025-03-20T07:35:09.121405",
          "status": "completed"
        },
        "tags": [],
        "trusted": true,
        "id": "f8347ebb"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_metrics(train_losses, train_accs, val_losses, val_accs):\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "    ax1.plot(train_losses, label='Train Loss')\n",
        "    ax1.plot(val_losses, label='Val Loss')\n",
        "    ax1.set_title('Loss vs Epoch')\n",
        "    ax1.set_xlabel('Epoch')\n",
        "    ax1.set_ylabel('Loss')\n",
        "    ax1.legend()\n",
        "\n",
        "    ax2.plot(train_accs, label='Train Accuracy')\n",
        "    ax2.plot(val_accs, label='Val Accuracy')\n",
        "    ax2.set_title('Accuracy vs Epoch')\n",
        "    ax2.set_xlabel('Epoch')\n",
        "    ax2.set_ylabel('Accuracy (%)')\n",
        "    ax2.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-03-22T13:54:42.256659Z",
          "iopub.execute_input": "2025-03-22T13:54:42.25692Z",
          "iopub.status.idle": "2025-03-22T13:54:42.271251Z",
          "shell.execute_reply.started": "2025-03-22T13:54:42.256896Z",
          "shell.execute_reply": "2025-03-22T13:54:42.270363Z"
        },
        "papermill": {
          "duration": 0.014198,
          "end_time": "2025-03-20T07:35:09.158372",
          "exception": false,
          "start_time": "2025-03-20T07:35:09.144174",
          "status": "completed"
        },
        "tags": [],
        "trusted": true,
        "id": "60dcba37"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def print_and_get_metrics(technique, epoch, metrics, model_size, flops, *args, **kwargs):\n",
        "    print(\"\\nFinal Model Performance:\")\n",
        "\n",
        "    print(f\"Result of {technique}:\")\n",
        "    print(f\"Total epochs: {epoch}\")\n",
        "    print(f\"Model Size: {model_size:.2f} MB\")\n",
        "    print(f\"FLOPs: {flops / 1e9:.2f} GFLOPs\")\n",
        "    print(f\"Accuracy: {metrics['acc']:.2f}%\")\n",
        "    print(f\"Top-1 Accuracy: {metrics['top1_acc']:.2f}%\")\n",
        "    print(f\"Top-5 Accuracy: {metrics['top5_acc']:.2f}%\")\n",
        "\n",
        "    result = {\n",
        "        \"technique\": technique,\n",
        "        \"epoch\": epoch,\n",
        "        \"model_size\": f\"{model_size:.2f} MB\",\n",
        "        \"flops\": f\"{flops / 1e9:.2f} GFLOPs\",\n",
        "        \"acc\": f\"{metrics['acc']:.2f}%\",\n",
        "        \"top1\": f\"{metrics['top1_acc']:.2f}%\",\n",
        "        \"top5\": f\"{metrics['top5_acc']:.2f}%\",\n",
        "    }\n",
        "\n",
        "    for extra in args:\n",
        "        if isinstance(extra, dict):\n",
        "            result.update(extra)\n",
        "\n",
        "    result.update(kwargs)\n",
        "\n",
        "    return result"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-03-22T13:54:42.272069Z",
          "iopub.execute_input": "2025-03-22T13:54:42.272363Z",
          "iopub.status.idle": "2025-03-22T13:54:42.293048Z",
          "shell.execute_reply.started": "2025-03-22T13:54:42.272333Z",
          "shell.execute_reply": "2025-03-22T13:54:42.292117Z"
        },
        "papermill": {
          "duration": 0.014746,
          "end_time": "2025-03-20T07:35:09.202728",
          "exception": false,
          "start_time": "2025-03-20T07:35:09.187982",
          "status": "completed"
        },
        "tags": [],
        "trusted": true,
        "id": "4e638445"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Baseline Model Training"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.00816,
          "end_time": "2025-03-20T07:35:09.219208",
          "exception": false,
          "start_time": "2025-03-20T07:35:09.211048",
          "status": "completed"
        },
        "tags": [],
        "id": "c56c75fb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration\n",
        "num_epochs=30\n",
        "batch_size=32\n",
        "num_samples=5000\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Dataset details\n",
        "print(f\"\\nDataset Information:\")\n",
        "print(f\"Training samples: {num_samples}\")\n",
        "print(f\"Validation samples: {num_samples // 5}\")\n",
        "print(f\"Batch size: {batch_size}\")\n",
        "train_loader, val_loader = get_data_loaders(num_samples, batch_size)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-03-22T13:54:42.297354Z",
          "iopub.execute_input": "2025-03-22T13:54:42.297658Z",
          "iopub.status.idle": "2025-03-22T13:54:43.95771Z",
          "shell.execute_reply.started": "2025-03-22T13:54:42.297621Z",
          "shell.execute_reply": "2025-03-22T13:54:43.956758Z"
        },
        "papermill": {
          "duration": 0.0573,
          "end_time": "2025-03-20T07:35:09.284525",
          "exception": false,
          "start_time": "2025-03-20T07:35:09.227225",
          "status": "completed"
        },
        "tags": [],
        "trusted": true,
        "id": "e90ad02a"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def run_baseline(num_epochs, batch_size, num_samples):\n",
        "    model = get_model().to(device)\n",
        "\n",
        "    print(\"\\nModel Summary:\")\n",
        "    print(summary(model, input_size=(batch_size, 3, 224, 224)))\n",
        "\n",
        "    model_size = sum(p.numel() * p.element_size() for p in model.parameters()) / (1024 * 1024)\n",
        "    flops = count_flops(model)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "    train_losses, train_accs = [], []\n",
        "    val_losses, val_accs = [], []\n",
        "\n",
        "    early_stopping = EarlyStopping(patience=3, min_delta=0.001)\n",
        "\n",
        "    print(\"\\nStarting training Baseline...\")\n",
        "    epochs_performed = 0\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
        "\n",
        "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "        train_losses.append(train_loss)\n",
        "        train_accs.append(train_acc)\n",
        "\n",
        "        eval_metrics = evaluate(model, val_loader, criterion, device)\n",
        "        val_losses.append(eval_metrics['loss'])\n",
        "        val_accs.append(eval_metrics['acc'])\n",
        "        epochs_performed = epochs_performed + 1\n",
        "        # Early stopping logic\n",
        "        early_stopping(eval_metrics['loss'], model)\n",
        "        if early_stopping.early_stop:\n",
        "            print(\"Early stopping triggered\")\n",
        "            break\n",
        "\n",
        "    plot_metrics(train_losses, train_accs, val_losses, val_accs)\n",
        "\n",
        "    model = early_stopping.best_model\n",
        "    scripted_model = torch.jit.script(model)\n",
        "    scripted_model.save(\"baseline_cifar10.pt\")\n",
        "\n",
        "    print(\"\\nPerforming final evaluation for Baseline cifar10...\")\n",
        "    gpu_metrics = evaluate(model, val_loader, criterion, device)\n",
        "\n",
        "    result = print_and_get_metrics(\"Baseline - CIFAR-10\", epochs_performed, gpu_metrics,  model_size, flops)\n",
        "\n",
        "    return model, result"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-03-22T13:54:43.959505Z",
          "iopub.execute_input": "2025-03-22T13:54:43.959762Z",
          "iopub.status.idle": "2025-03-22T13:54:43.967363Z",
          "shell.execute_reply.started": "2025-03-22T13:54:43.95974Z",
          "shell.execute_reply": "2025-03-22T13:54:43.966515Z"
        },
        "papermill": {
          "duration": 0.017324,
          "end_time": "2025-03-20T07:35:09.310297",
          "exception": false,
          "start_time": "2025-03-20T07:35:09.292973",
          "status": "completed"
        },
        "tags": [],
        "trusted": true,
        "id": "0b825401"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "baseline_cifar10, baseline_result = run_baseline(num_epochs, batch_size, num_samples)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-03-22T13:54:43.968285Z",
          "iopub.execute_input": "2025-03-22T13:54:43.968488Z",
          "iopub.status.idle": "2025-03-22T13:58:49.120234Z",
          "shell.execute_reply.started": "2025-03-22T13:54:43.968472Z",
          "shell.execute_reply": "2025-03-22T13:58:49.119206Z"
        },
        "papermill": {
          "duration": 837.345102,
          "end_time": "2025-03-20T07:49:06.663815",
          "exception": false,
          "start_time": "2025-03-20T07:35:09.318713",
          "status": "completed"
        },
        "tags": [],
        "trusted": true,
        "id": "2a20610f"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Pruning\n",
        "\n",
        "## Structured Pruning"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.140097,
          "end_time": "2025-03-20T07:49:06.944559",
          "exception": false,
          "start_time": "2025-03-20T07:49:06.804462",
          "status": "completed"
        },
        "tags": [],
        "id": "cdb9ae47"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ptflops\n",
        "import torch.nn.utils.prune as prune\n",
        "from ptflops import get_model_complexity_info"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-03-22T13:58:49.121432Z",
          "iopub.execute_input": "2025-03-22T13:58:49.121752Z",
          "iopub.status.idle": "2025-03-22T13:58:53.585898Z",
          "shell.execute_reply.started": "2025-03-22T13:58:49.121726Z",
          "shell.execute_reply": "2025-03-22T13:58:53.585199Z"
        },
        "papermill": {
          "duration": 7.197311,
          "end_time": "2025-03-20T07:49:14.284327",
          "exception": false,
          "start_time": "2025-03-20T07:49:07.087016",
          "status": "completed"
        },
        "tags": [],
        "trusted": true,
        "id": "421f59d0"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### StructuredPruner Class for Convolutional Neural Networks\n",
        "\n",
        "The `StructuredPruner` class implements **structured pruning** for convolutional neural networks, focusing on removing entire filters (or channels) based on their importance. This pruning technique reduces model complexity and improves inference speed while minimizing the impact on model performance.\n",
        "\n",
        "**Key Features:**\n",
        "- **Filter Importance Scoring:** Uses the **L1-norm** of convolutional filters to measure their importance, prioritizing the removal of less significant filters.\n",
        "- **Layer-Wise Pruning:** Prunes filters from convolutional layers and adjusts associated batch normalization layers to maintain consistency.\n",
        "- **Automatic Adjustment of Fully Connected Layers:** After pruning convolutional layers, the class adjusts the first fully connected layer to match the reduced feature map dimensions.\n",
        "- **Pruning History Tracking:** Records pruning statistics, including the number of filters before and after pruning for each layer.\n",
        "- **Flexible Pruning Ratio:** Allows users to specify the fraction of filters to prune, providing control over the trade-off between model size and accuracy.\n",
        "\n",
        "**Benefits of Structured Pruning:**\n",
        "- **Model Compression:** Reduces the number of parameters and model size for more efficient storage and deployment.\n",
        "- **Faster Inference:** Decreases computational load, leading to faster inference times, especially on resource-constrained devices.\n",
        "- **Energy Efficiency:** Reduces power consumption by lowering the number of computations.\n",
        "\n",
        "**Parameters:**\n",
        "- `model` (torch.nn.Module): The neural network model to be pruned.\n",
        "\n",
        "**Methods:**\n",
        "- `compute_filter_importance(conv_layer)`: Computes the importance score for each filter in a convolutional layer using the L1-norm.\n",
        "- `prune_conv_layer(conv_layer, bn_layer, prune_ratio)`: Prunes the least important filters from a convolutional layer and adjusts the batch normalization layer if present.\n",
        "- `prune_model(prune_ratio, input_shape=(3, 224, 224))`: Applies structured pruning across the entire model and adjusts subsequent layers accordingly.\n",
        "- `_adjust_fc_layer(input_shape)`: Adjusts the first fully connected (FC) layer to match the reduced output size from the convolutional layers after pruning.\n",
        "- `print_pruning_history()`: Displays a summary of the pruning operations performed on the model.\n",
        "\n",
        "**Returns:**\n",
        "- A pruned version of the original model with reduced parameters and computational complexity.\n"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.185477,
          "end_time": "2025-03-20T07:49:14.611408",
          "exception": false,
          "start_time": "2025-03-20T07:49:14.425931",
          "status": "completed"
        },
        "tags": [],
        "id": "02aec65a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class StructuredPruner:\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "        self.pruning_history = []\n",
        "\n",
        "    def compute_filter_importance(self, conv_layer):\n",
        "        \"\"\"Compute L1-norm of each filter as importance score\"\"\"\n",
        "        weights = conv_layer.weight.data\n",
        "        importance = torch.sum(torch.abs(weights.view(weights.size(0), -1)), dim=1)\n",
        "        return importance\n",
        "\n",
        "    def prune_conv_layer(self, conv_layer, bn_layer, prune_ratio):\n",
        "        n_filters = conv_layer.weight.size(0)\n",
        "        n_prune = int(n_filters * prune_ratio)\n",
        "\n",
        "        if n_prune == 0:\n",
        "            return torch.arange(n_filters)  # Return all indices if nothing is pruned\n",
        "\n",
        "        importance = self.compute_filter_importance(conv_layer)\n",
        "        _, indices = torch.sort(importance)\n",
        "        indices_to_keep = indices[n_prune:]\n",
        "\n",
        "        # Prune conv layer\n",
        "        conv_layer.weight.data = torch.index_select(conv_layer.weight.data, 0, indices_to_keep)\n",
        "        if conv_layer.bias is not None:\n",
        "            conv_layer.bias.data = torch.index_select(conv_layer.bias.data, 0, indices_to_keep)\n",
        "        conv_layer.out_channels = len(indices_to_keep)\n",
        "\n",
        "        # Prune batch norm layer if present\n",
        "        if bn_layer is not None:\n",
        "            bn_layer.weight.data = torch.index_select(bn_layer.weight.data, 0, indices_to_keep)\n",
        "            bn_layer.bias.data = torch.index_select(bn_layer.bias.data, 0, indices_to_keep)\n",
        "            bn_layer.running_mean = torch.index_select(bn_layer.running_mean, 0, indices_to_keep)\n",
        "            bn_layer.running_var = torch.index_select(bn_layer.running_var, 0, indices_to_keep)\n",
        "            bn_layer.num_features = len(indices_to_keep)\n",
        "\n",
        "        return indices_to_keep\n",
        "\n",
        "    def _pre_prune(self):\n",
        "        self.flops_original, self.params_original = get_model_complexity_info(\n",
        "            self.model, (3, 224, 224), as_strings=False, print_per_layer_stat=False\n",
        "        )\n",
        "        self.flops_original = f\"{self.flops_original / 1e9:.2f} GFLOPs\"\n",
        "        self.params_original = f\"{self.params_original / 1e6:.2f} Million\"\n",
        "        print(f\"Original Model: {self.flops_original}, {self.params_original}\")\n",
        "\n",
        "    def _post_prune(self):\n",
        "        self.flops_pruned, self.params_pruned = get_model_complexity_info(\n",
        "            self.model, (3, 224, 224), as_strings=False, print_per_layer_stat=False\n",
        "        )\n",
        "\n",
        "        self.flops_pruned = f\"{self.flops_pruned / 1e9:.2f} GFLOPs\"\n",
        "        self.params_pruned = f\"{self.params_pruned / 1e6:.2f} Million\"\n",
        "\n",
        "        print(f\"Pruned Model: {self.flops_pruned}, {self.params_pruned}\")\n",
        "\n",
        "\n",
        "    def prune_model(self, prune_ratio, input_shape=(3, 224, 224)):\n",
        "        self._pre_prune()\n",
        "        prev_indices = None\n",
        "\n",
        "        # Get all conv and batch norm layers\n",
        "        conv_layers = [module for module in self.model.features if isinstance(module, nn.Conv2d)]\n",
        "        bn_layers = [module for module in self.model.features if isinstance(module, nn.BatchNorm2d)]\n",
        "\n",
        "        print(f\"Found {len(conv_layers)} convolutional layers and {len(bn_layers)} batch norm layers.\")\n",
        "\n",
        "        for i, conv in enumerate(conv_layers):\n",
        "            bn = bn_layers[i] if i < len(bn_layers) else None\n",
        "            filters_before = conv.weight.size(0)\n",
        "\n",
        "            # Prune current layer\n",
        "            indices = self.prune_conv_layer(conv, bn, prune_ratio)\n",
        "\n",
        "            # Adjust input channels of next conv layer based on pruned channels\n",
        "            if prev_indices is not None:\n",
        "                conv.weight.data = torch.index_select(conv.weight.data, 1, prev_indices)\n",
        "                conv.in_channels = len(prev_indices)\n",
        "\n",
        "            prev_indices = indices\n",
        "\n",
        "            # Record pruning statistics\n",
        "            self.pruning_history.append({\n",
        "                'layer': i,\n",
        "                'filters_before': filters_before,\n",
        "                'filters_after': len(indices) if indices is not None else filters_before\n",
        "            })\n",
        "\n",
        "        # Adjust the first FC layer based on the new conv output\n",
        "        self._adjust_fc_layer(input_shape)\n",
        "        self._post_prune()\n",
        "\n",
        "    def _adjust_fc_layer(self, input_shape):\n",
        "        # Run a dummy input through the model to determine new feature size\n",
        "        dummy_input = torch.randn(1, *input_shape)\n",
        "        with torch.no_grad():\n",
        "            features = self.model.features(dummy_input)\n",
        "            flattened_size = features.view(1, -1).size(1)\n",
        "\n",
        "        # Update the FC layer to match the new flattened size\n",
        "        fc = self.model.classifier[0]\n",
        "        fc.in_features = flattened_size\n",
        "        fc.weight.data = fc.weight.data[:, :flattened_size]  # Adjust weights\n",
        "        print(f\"Adjusted FC layer input size to: {flattened_size}\")\n",
        "\n",
        "    def get_pruned_stats(self):\n",
        "        return {\n",
        "            'flops_original': self.flops_original,\n",
        "            'flops_pruned': self.flops_pruned,\n",
        "            'params_original': self.params_original,\n",
        "            'params_pruned': self.params_pruned\n",
        "        }\n",
        "\n",
        "    def print_pruning_history(self):\n",
        "        if not self.pruning_history:\n",
        "            print(\"No pruning has been recorded.\")\n",
        "        for entry in self.pruning_history:\n",
        "            print(f\"Layer {entry['layer']}: Filters before = {entry['filters_before']}, Filters after = {entry['filters_after']}\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-03-22T13:58:53.586712Z",
          "iopub.execute_input": "2025-03-22T13:58:53.586921Z",
          "iopub.status.idle": "2025-03-22T13:58:53.600472Z",
          "shell.execute_reply.started": "2025-03-22T13:58:53.586902Z",
          "shell.execute_reply": "2025-03-22T13:58:53.599568Z"
        },
        "papermill": {
          "duration": 0.156404,
          "end_time": "2025-03-20T07:49:14.907198",
          "exception": false,
          "start_time": "2025-03-20T07:49:14.750794",
          "status": "completed"
        },
        "tags": [],
        "trusted": true,
        "jupyter": {
          "source_hidden": true
        },
        "id": "64495e01"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### VGG19 Model with Structured Pruning\n",
        "\n",
        "This function initializes a pre-trained **VGG19** model, modifies it for a specified number of classes, and applies **structured pruning** to reduce the model's complexity. The pruning process removes less important filters from convolutional layers, optimizing the model for faster inference and reduced memory usage.\n",
        "\n",
        "**Key Features:**\n",
        "- **Pre-trained VGG19 Backbone:** Loads a VGG19 model pre-trained on ImageNet and modifies the final classification layer for the target dataset.\n",
        "- **Structured Pruning Integration:** Applies structured pruning using the `StructuredPruner` class, which removes less important filters based on their L1-norm importance scores.\n",
        "- **Customizable Pruning Ratio:** Allows users to define the proportion of filters to prune from each convolutional layer.\n",
        "- **Pruning History Tracking:** Returns the pruner object, which contains a history of pruning operations, enabling analysis of model modifications.\n",
        "\n",
        "**Benefits of Structured Pruning:**\n",
        "- **Model Compression:** Reduces the number of parameters, leading to a smaller model size and faster inference times.\n",
        "- **Performance Optimization:** Optimized for deployment on resource-constrained devices, such as mobile and embedded systems.\n",
        "- **Energy Efficiency:** Reduces computational requirements, lowering energy consumption during inference.\n"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.140482,
          "end_time": "2025-03-20T07:49:15.18681",
          "exception": false,
          "start_time": "2025-03-20T07:49:15.046328",
          "status": "completed"
        },
        "tags": [],
        "id": "3bc96ca1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model_with_pruning(classes=10, prune_ratio=0.3):\n",
        "    model = get_model(classes)\n",
        "    pruner = StructuredPruner(model)\n",
        "    pruner.prune_model(prune_ratio)\n",
        "    return model, pruner"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-03-22T13:58:53.601346Z",
          "iopub.execute_input": "2025-03-22T13:58:53.601538Z",
          "iopub.status.idle": "2025-03-22T13:58:53.623012Z",
          "shell.execute_reply.started": "2025-03-22T13:58:53.60152Z",
          "shell.execute_reply": "2025-03-22T13:58:53.622152Z"
        },
        "papermill": {
          "duration": 0.146582,
          "end_time": "2025-03-20T07:49:15.472375",
          "exception": false,
          "start_time": "2025-03-20T07:49:15.325793",
          "status": "completed"
        },
        "tags": [],
        "trusted": true,
        "id": "f99e4593"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def run_structured_pruning(num_epochs, batch_size, num_samples, prune_ratio=0.3):\n",
        "\n",
        "    model, pruner = get_model_with_pruning(classes=10, prune_ratio=prune_ratio)\n",
        "    model = model.to(device)\n",
        "\n",
        "    print(\"\\nModel Summary after structured pruning:\")\n",
        "    print(summary(model, input_size=(batch_size, 3, 224, 224)))\n",
        "    print(\"\\nPruning Statistics:\")\n",
        "    pruner.print_pruning_history()\n",
        "    stats = pruner.get_pruned_stats()\n",
        "\n",
        "    model_size = sum(p.numel() * p.element_size() for p in model.parameters()) / (1024 * 1024)\n",
        "    flops = count_flops(model)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "    train_losses, train_accs = [], []\n",
        "    val_losses, val_accs = [], []\n",
        "\n",
        "    early_stopping = EarlyStopping(patience=5, min_delta=0.001)\n",
        "\n",
        "    # Training loop\n",
        "    print(\"\\nStarting training structure pruned model...\")\n",
        "    epochs_performed = 0\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
        "\n",
        "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "        train_losses.append(train_loss)\n",
        "        train_accs.append(train_acc)\n",
        "\n",
        "        eval_metrics = evaluate(model, val_loader, criterion, device)\n",
        "        val_losses.append(eval_metrics['loss'])\n",
        "        val_accs.append(eval_metrics['acc'])\n",
        "        epochs_performed = epochs_performed + 1\n",
        "        early_stopping(eval_metrics['loss'], model)\n",
        "        if early_stopping.early_stop:\n",
        "            print(\"Early stopping triggered\")\n",
        "            break\n",
        "\n",
        "    plot_metrics(train_losses, train_accs, val_losses, val_accs)\n",
        "\n",
        "    model = early_stopping.best_model\n",
        "    scripted_model = torch.jit.script(model)\n",
        "    scripted_model.save(\"sp_cifar10.pt\")\n",
        "\n",
        "    print(\"\\nPerforming final evaluation for Structured cifar10...\")\n",
        "    gpu_metrics = evaluate(model, val_loader, criterion, device)\n",
        "\n",
        "    result = print_and_get_metrics(\n",
        "        \"Structured Pruning - CIFAR-10\", epochs_performed,\n",
        "        gpu_metrics, model_size, flops, stats\n",
        "    )\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    return model, result"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-03-22T13:58:53.623896Z",
          "iopub.execute_input": "2025-03-22T13:58:53.6241Z",
          "iopub.status.idle": "2025-03-22T13:58:53.638208Z",
          "shell.execute_reply.started": "2025-03-22T13:58:53.624082Z",
          "shell.execute_reply": "2025-03-22T13:58:53.637614Z"
        },
        "papermill": {
          "duration": 0.191669,
          "end_time": "2025-03-20T07:49:15.80791",
          "exception": false,
          "start_time": "2025-03-20T07:49:15.616241",
          "status": "completed"
        },
        "tags": [],
        "trusted": true,
        "id": "540ba974"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "sp_cifar10, sp_result = run_structured_pruning(num_epochs, batch_size, num_samples)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-03-22T13:58:53.638989Z",
          "iopub.execute_input": "2025-03-22T13:58:53.639236Z",
          "iopub.status.idle": "2025-03-22T14:02:43.277519Z",
          "shell.execute_reply.started": "2025-03-22T13:58:53.639205Z",
          "shell.execute_reply": "2025-03-22T14:02:43.276579Z"
        },
        "papermill": {
          "duration": 1235.589271,
          "end_time": "2025-03-20T08:09:51.535504",
          "exception": false,
          "start_time": "2025-03-20T07:49:15.946233",
          "status": "completed"
        },
        "tags": [],
        "trusted": true,
        "id": "e4626b93"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unstructured Pruning"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.360313,
          "end_time": "2025-03-20T08:09:52.306776",
          "exception": false,
          "start_time": "2025-03-20T08:09:51.946463",
          "status": "completed"
        },
        "tags": [],
        "id": "8e8ce50b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim.lr_scheduler import ReduceLROnPlateau"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-03-22T14:02:43.278848Z",
          "iopub.execute_input": "2025-03-22T14:02:43.279137Z",
          "iopub.status.idle": "2025-03-22T14:02:43.282717Z",
          "shell.execute_reply.started": "2025-03-22T14:02:43.279113Z",
          "shell.execute_reply": "2025-03-22T14:02:43.281899Z"
        },
        "papermill": {
          "duration": 0.460175,
          "end_time": "2025-03-20T08:09:53.125197",
          "exception": false,
          "start_time": "2025-03-20T08:09:52.665022",
          "status": "completed"
        },
        "tags": [],
        "trusted": true,
        "id": "453b3d78"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### UnstructuredPruner Class for Neural Network Pruning\n",
        "\n",
        "The `UnstructuredPruner` class implements **unstructured pruning** for neural networks, which selectively removes individual weights based on their importance (typically by magnitude). This fine-grained pruning method allows for more flexible reduction of model parameters without altering the network's structure.\n",
        "\n",
        "**Key Features:**\n",
        "- **L1-Norm Unstructured Pruning:** Prunes weights with the smallest absolute values using **L1-norm** as the criterion, targeting less significant connections.\n",
        "- **Gradual Pruning:** Supports gradual pruning across multiple steps, incrementally increasing sparsity to reduce the risk of accuracy degradation.\n",
        "- **Sparsity Tracking:** Calculates and tracks the sparsity (percentage of zero weights) for each pruned layer, providing detailed pruning statistics.\n",
        "- **Permanent Pruning:** Removes the pruning reparametrization, making the weight removals permanent and reducing computational overhead.\n",
        "- **Pruning Summary Reporting:** Provides comprehensive statistics, including total parameters, remaining parameters, and overall sparsity.\n",
        "\n",
        "**Benefits of Unstructured Pruning:**\n",
        "- **Fine-Grained Control:** Allows selective pruning at the individual weight level, providing greater flexibility compared to structured pruning.\n",
        "- **Model Compression:** Reduces the number of active parameters, leading to smaller models and faster inference times.\n",
        "- **Performance Optimization:** Can improve model efficiency, especially on hardware that supports sparse matrix operations.\n"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.355974,
          "end_time": "2025-03-20T08:09:53.839564",
          "exception": false,
          "start_time": "2025-03-20T08:09:53.48359",
          "status": "completed"
        },
        "tags": [],
        "id": "cc9f64d1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class UnstructuredPruner:\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "        self.pruning_stats = {}\n",
        "\n",
        "    def calculate_sparsity(self, module):\n",
        "        total_params = module.weight.nelement()\n",
        "        zero_params = torch.sum(module.weight == 0).item()\n",
        "        return (zero_params / total_params) * 100\n",
        "\n",
        "    def apply_gradual_pruning(self, initial_amount=0.1, final_amount=0.3, steps=3):\n",
        "        step_amount = (final_amount - initial_amount) / steps\n",
        "        current_amount = initial_amount\n",
        "\n",
        "        for step in range(steps):\n",
        "            print(f\"\\nApplying pruning step {step + 1}/{steps} (amount: {current_amount:.2f})\")\n",
        "            for name, module in self.model.named_modules():\n",
        "                if isinstance(module, (nn.Conv2d, nn.Linear)):\n",
        "                    prune.l1_unstructured(module, name='weight', amount=current_amount)\n",
        "            current_amount += step_amount\n",
        "            self.update_pruning_stats()\n",
        "\n",
        "    def update_pruning_stats(self):\n",
        "        for name, module in self.model.named_modules():\n",
        "            if isinstance(module, (nn.Conv2d, nn.Linear)):\n",
        "                self.pruning_stats[name] = {\n",
        "                    'sparsity': self.calculate_sparsity(module),\n",
        "                    'total_params': module.weight.nelement(),\n",
        "                    'remaining_params': torch.sum(module.weight != 0).item()\n",
        "                }\n",
        "\n",
        "    def make_permanent(self):\n",
        "        \"\"\"Remove pruning reparametrization and make pruning permanent\"\"\"\n",
        "        for module in self.model.modules():\n",
        "            if isinstance(module, (nn.Conv2d, nn.Linear)):\n",
        "                prune.remove(module, 'weight')\n",
        "\n",
        "    def get_pruning_summary(self):\n",
        "        total_params = 0\n",
        "        remaining_params = 0\n",
        "\n",
        "        for stats in self.pruning_stats.values():\n",
        "            total_params += stats['total_params']\n",
        "            remaining_params += stats['remaining_params']\n",
        "\n",
        "        return {\n",
        "            'total_params': total_params,\n",
        "            'remaining_params': remaining_params,\n",
        "            'overall_sparsity': ((total_params - remaining_params) / total_params) * 100,\n",
        "            'layer_stats': self.pruning_stats\n",
        "        }\n",
        "\n",
        "    def print_stats(self):\n",
        "        pruning_summary = self.get_pruning_summary()\n",
        "        print(f\"Total parameters: {pruning_summary['total_params']:,}\")\n",
        "        print(f\"Remaining parameters: {pruning_summary['remaining_params']:,}\")\n",
        "        pr = f\"{((pruning_summary['total_params'] - pruning_summary['remaining_params']) / pruning_summary['total_params'] * 100):.2f}%\"\n",
        "        print(f\"Parameters Reduction: {pr}\")\n",
        "        print(f\"Overall sparsity: {pruning_summary['overall_sparsity']:.2f}%\")\n",
        "\n",
        "        for feature, stats in self.pruning_stats.items():\n",
        "            print(feature , f\": Total params: {stats['total_params']},  Remaining params: {stats['remaining_params']}, Sparsity: {stats['sparsity']}\")\n",
        "\n",
        "        return {\n",
        "            'parameter_reduction': pr,\n",
        "            'overall_sparsity': f\"{pruning_summary['overall_sparsity']:.2f}%\"\n",
        "        }"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-03-22T14:02:43.283698Z",
          "iopub.execute_input": "2025-03-22T14:02:43.283921Z",
          "iopub.status.idle": "2025-03-22T14:02:43.303732Z",
          "shell.execute_reply.started": "2025-03-22T14:02:43.283902Z",
          "shell.execute_reply": "2025-03-22T14:02:43.30302Z"
        },
        "papermill": {
          "duration": 0.415801,
          "end_time": "2025-03-20T08:09:54.613847",
          "exception": false,
          "start_time": "2025-03-20T08:09:54.198046",
          "status": "completed"
        },
        "tags": [],
        "trusted": true,
        "id": "58293aa1"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### VGG19 Model with Unstructured Pruning\n",
        "\n",
        "This function initializes a **VGG19** model with pretrained weights, modifies the final classification layer, and applies **unstructured pruning** to reduce model complexity. Unstructured pruning removes individual weights with the least importance, leading to a sparse model that maintains accuracy while reducing the number of active parameters.\n",
        "\n",
        "**Key Features:**\n",
        "- **Pretrained VGG19 Backbone:** Loads a VGG19 model pre-trained on ImageNet and modifies the final classification layer for the specified number of classes.\n",
        "- **Unstructured Pruning:** Applies gradual unstructured pruning using the `UnstructuredPruner` class, removing individual weights based on their L1-norm.\n",
        "- **Gradual Pruning Steps:** Prunes the model incrementally across multiple steps, reducing the risk of sudden performance degradation.\n",
        "- **Permanent Pruning:** Removes the pruning reparametrization, making the weight removals permanent and optimizing inference performance.\n",
        "\n",
        "**Benefits of Unstructured Pruning:**\n",
        "- **Fine-Grained Compression:** Provides flexible parameter reduction without altering the architecture of the network.\n",
        "- **Inference Efficiency:** Reduces the number of computations, which can speed up inference on hardware that supports sparse matrix operations.\n",
        "- **Energy and Memory Savings:** Reduces the computational load and memory footprint of the model, making it ideal for deployment on resource-constrained devices.\n"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.359601,
          "end_time": "2025-03-20T08:09:55.327647",
          "exception": false,
          "start_time": "2025-03-20T08:09:54.968046",
          "status": "completed"
        },
        "tags": [],
        "id": "cac95cb6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model_with_unstructured_pruning(classes=10, prune_amount=0.3):\n",
        "    # Initialize model with pretrained weights\n",
        "    model = get_model(classes)\n",
        "\n",
        "    # Initialize the new classifier layer properly\n",
        "    nn.init.kaiming_normal_(model.classifier[6].weight)\n",
        "    nn.init.constant_(model.classifier[6].bias, 0)\n",
        "\n",
        "    # Apply gradual pruning\n",
        "    pruner = UnstructuredPruner(model)\n",
        "    pruner.apply_gradual_pruning(initial_amount=0.1, final_amount=prune_amount, steps=3)\n",
        "    pruner.make_permanent()\n",
        "\n",
        "    return model, pruner"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-03-22T14:02:43.30441Z",
          "iopub.execute_input": "2025-03-22T14:02:43.304655Z",
          "iopub.status.idle": "2025-03-22T14:02:43.323153Z",
          "shell.execute_reply.started": "2025-03-22T14:02:43.304635Z",
          "shell.execute_reply": "2025-03-22T14:02:43.322582Z"
        },
        "papermill": {
          "duration": 0.401028,
          "end_time": "2025-03-20T08:09:56.133416",
          "exception": false,
          "start_time": "2025-03-20T08:09:55.732388",
          "status": "completed"
        },
        "tags": [],
        "trusted": true,
        "id": "ecdde2c9"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def run_unstructured_pruning(num_epochs, batch_size, num_samples):\n",
        "    model, pruner = get_model_with_unstructured_pruning()\n",
        "    model = model.to(device)\n",
        "\n",
        "    model_size = sum(p.numel() * p.element_size() for p in model.parameters()) / (1024 * 1024)\n",
        "    flops = count_flops(model)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2, verbose=True)\n",
        "\n",
        "    print(\"\\nPruning Statistics:\")\n",
        "    pruning_summary = pruner.print_stats()\n",
        "\n",
        "    train_losses, train_accs = [], []\n",
        "    val_losses, val_accs = [], []\n",
        "\n",
        "    early_stopping = EarlyStopping(patience=5, min_delta=0.001)\n",
        "\n",
        "    # Training loop\n",
        "    print(\"\\nStarting training unstructure pruned model...\")\n",
        "    epochs_performed = 0\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
        "\n",
        "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "        train_losses.append(train_loss)\n",
        "        train_accs.append(train_acc)\n",
        "\n",
        "        eval_metrics = evaluate(model, val_loader, criterion, device)\n",
        "        val_losses.append(eval_metrics['loss'])\n",
        "        val_accs.append(eval_metrics['acc'])\n",
        "\n",
        "        # Learning rate scheduling\n",
        "        scheduler.step(eval_metrics['loss'])\n",
        "        epochs_performed = epochs_performed + 1\n",
        "        # Early stopping logic\n",
        "        early_stopping(eval_metrics['loss'], model)\n",
        "        if early_stopping.early_stop:\n",
        "            print(\"Early stopping triggered\")\n",
        "            break\n",
        "\n",
        "\n",
        "    plot_metrics(train_losses, train_accs, val_losses, val_accs)\n",
        "\n",
        "    model = early_stopping.best_model\n",
        "    scripted_model = torch.jit.script(model)\n",
        "    scripted_model.save(\"usp_cifar10.pt\")\n",
        "\n",
        "    print(\"\\nPerforming final evaluation for Unstructured cifar10...\")\n",
        "    gpu_metrics = evaluate(model, val_loader, criterion, device)\n",
        "\n",
        "    result = print_and_get_metrics(\n",
        "        \"Unstructured Pruning - CIFAR-10\",\n",
        "        epochs_performed, gpu_metrics, model_size, flops,\n",
        "    )\n",
        "\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    return model, result"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-03-22T14:02:43.324421Z",
          "iopub.execute_input": "2025-03-22T14:02:43.324728Z",
          "iopub.status.idle": "2025-03-22T14:02:43.341469Z",
          "shell.execute_reply.started": "2025-03-22T14:02:43.324699Z",
          "shell.execute_reply": "2025-03-22T14:02:43.340817Z"
        },
        "papermill": {
          "duration": 0.383205,
          "end_time": "2025-03-20T08:09:56.90746",
          "exception": false,
          "start_time": "2025-03-20T08:09:56.524255",
          "status": "completed"
        },
        "tags": [],
        "trusted": true,
        "id": "9ee8f0a2"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "usp_cifar10, usp_result = run_unstructured_pruning(num_epochs, batch_size, num_samples)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-03-22T14:02:43.34221Z",
          "iopub.execute_input": "2025-03-22T14:02:43.342459Z",
          "iopub.status.idle": "2025-03-22T14:07:24.772458Z",
          "shell.execute_reply.started": "2025-03-22T14:02:43.34244Z",
          "shell.execute_reply": "2025-03-22T14:07:24.771778Z"
        },
        "papermill": {
          "duration": 906.86036,
          "end_time": "2025-03-20T08:25:04.181872",
          "exception": false,
          "start_time": "2025-03-20T08:09:57.321512",
          "status": "completed"
        },
        "tags": [],
        "trusted": true,
        "id": "0a2d3462"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def comparison_diagram_extended(baseline_model, pruned_model, layer_name, pruning_type):\n",
        "    baseline_weights = dict(baseline_model.named_parameters())[layer_name].detach().cpu().numpy().flatten()\n",
        "    pruned_weights   = dict(pruned_model.named_parameters())[layer_name].detach().cpu().numpy().flatten()\n",
        "\n",
        "    # Compute L2 norm.\n",
        "    l2_baseline = np.linalg.norm(baseline_weights)\n",
        "    l2_pruned   = np.linalg.norm(pruned_weights)\n",
        "\n",
        "    # Compute L1 norm.\n",
        "    l1_baseline = np.sum(np.abs(baseline_weights))\n",
        "    l1_pruned   = np.sum(np.abs(pruned_weights))\n",
        "\n",
        "    # Parameter count.\n",
        "    param_count_baseline = baseline_weights.size\n",
        "    param_count_pruned   = pruned_weights.size\n",
        "\n",
        "    # Create a 1x4 figure.\n",
        "    fig, axs = plt.subplots(1, 4, figsize=(22, 5))\n",
        "\n",
        "    # Panel 1: Weight Distribution Histogram.\n",
        "    axs[0].hist(baseline_weights, bins=30, alpha=0.6, label='Baseline', color='blue')\n",
        "    axs[0].hist(pruned_weights, bins=30, alpha=0.6, label='Pruned', color='green')\n",
        "    axs[0].set_title('Weight Distribution')\n",
        "    axs[0].set_xlabel('Weight values')\n",
        "    axs[0].set_ylabel('Frequency')\n",
        "    axs[0].legend()\n",
        "\n",
        "    # Panel 2: L2 Norm Comparison.\n",
        "    axs[1].bar(['Baseline', 'Pruned'], [l2_baseline, l2_pruned], color=['blue', 'green'])\n",
        "    axs[1].set_title('L2 Norm')\n",
        "    axs[1].set_ylabel('L2 Norm')\n",
        "\n",
        "    # Panel 3: L1 Norm Comparison.\n",
        "    axs[2].bar(['Baseline', 'Pruned'], [l1_baseline, l1_pruned], color=['blue', 'green'])\n",
        "    axs[2].set_title('L1 Norm')\n",
        "    axs[2].set_ylabel('L1 Norm')\n",
        "\n",
        "    # Panel 4: Parameter Count Comparison.\n",
        "    axs[3].bar(['Baseline', 'Pruned'], [param_count_baseline, param_count_pruned], color=['blue', 'green'])\n",
        "    axs[3].set_title('Parameter Count')\n",
        "    axs[3].set_ylabel('Count')\n",
        "\n",
        "    plt.suptitle(f'Layer Comparison of {pruning_type} : {layer_name}', fontsize=16)\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
        "    plt.show()\n",
        "\n",
        "def find_max_improved_layer(baseline_model, pruned_model, pruning_type):\n",
        "    max_composite = -np.inf\n",
        "    max_layer = None\n",
        "    for name, param in baseline_model.named_parameters():\n",
        "        if 'weight' not in name:\n",
        "            continue\n",
        "        try:\n",
        "            base_w = param.detach().cpu().numpy().flatten()\n",
        "            pruned_w = dict(pruned_model.named_parameters())[name].detach().cpu().numpy().flatten()\n",
        "        except KeyError:\n",
        "            continue\n",
        "\n",
        "        # Compute metrics.\n",
        "        l2_base = np.linalg.norm(base_w)\n",
        "        l2_prun = np.linalg.norm(pruned_w)\n",
        "        l1_base = np.sum(np.abs(base_w))\n",
        "        l1_prun = np.sum(np.abs(pruned_w))\n",
        "        count_base = base_w.size\n",
        "        count_prun = pruned_w.size\n",
        "\n",
        "        # Avoid division by zero.\n",
        "        if l2_base == 0 or l1_base == 0:\n",
        "            continue\n",
        "\n",
        "        l2_improve = (l2_base - l2_prun) / l2_base\n",
        "        l1_improve = (l1_base - l1_prun) / l1_base\n",
        "\n",
        "        if pruning_type == \"structured\":\n",
        "            if count_base == 0:\n",
        "                continue\n",
        "            param_improve = (count_base - count_prun) / count_base\n",
        "            composite = (l2_improve + l1_improve + param_improve) / 3.0\n",
        "        else:\n",
        "            composite = (l2_improve + l1_improve) / 2.0\n",
        "\n",
        "        if composite > max_composite:\n",
        "            max_composite = composite\n",
        "            max_layer = name\n",
        "\n",
        "    return max_layer, max_composite\n",
        "\n",
        "def comparison_diagram_max_improved_layer(baseline_model, pruned_model, pruning_type):\n",
        "    max_layer, composite = find_max_improved_layer(baseline_model, pruned_model, pruning_type)\n",
        "    if max_layer is None:\n",
        "        print(\"No layer found with improvement.\")\n",
        "    else:\n",
        "        print(f\"Maximum improved layer: {max_layer}\\nComposite Improvement: {composite:.3f}\")\n",
        "        comparison_diagram_extended(baseline_model, pruned_model, max_layer, pruning_type)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-03-22T14:07:24.773497Z",
          "iopub.execute_input": "2025-03-22T14:07:24.773751Z",
          "iopub.status.idle": "2025-03-22T14:07:24.786463Z",
          "shell.execute_reply.started": "2025-03-22T14:07:24.773729Z",
          "shell.execute_reply": "2025-03-22T14:07:24.785631Z"
        },
        "papermill": {
          "duration": 0.551556,
          "end_time": "2025-03-20T08:25:05.226369",
          "exception": false,
          "start_time": "2025-03-20T08:25:04.674813",
          "status": "completed"
        },
        "tags": [],
        "trusted": true,
        "jupyter": {
          "source_hidden": true
        },
        "id": "c7c15532"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "comparison_diagram_max_improved_layer(baseline_cifar10, sp_cifar10,  'CIFAR-10 - Structured')\n",
        "comparison_diagram_max_improved_layer(baseline_cifar10, usp_cifar10, 'CIFAR-10 - Utructured')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-03-22T14:07:24.787216Z",
          "iopub.execute_input": "2025-03-22T14:07:24.78749Z",
          "iopub.status.idle": "2025-03-22T14:07:29.259375Z",
          "shell.execute_reply.started": "2025-03-22T14:07:24.787471Z",
          "shell.execute_reply": "2025-03-22T14:07:29.258511Z"
        },
        "papermill": {
          "duration": 4.976172,
          "end_time": "2025-03-20T08:25:10.689612",
          "exception": false,
          "start_time": "2025-03-20T08:25:05.71344",
          "status": "completed"
        },
        "tags": [],
        "trusted": true,
        "id": "404d8a57"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Qunatization"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.501155,
          "end_time": "2025-03-20T08:25:11.68095",
          "exception": false,
          "start_time": "2025-03-20T08:25:11.179795",
          "status": "completed"
        },
        "tags": [],
        "id": "8f8627f5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.quantization import QuantStub, DeQuantStub, prepare_qat, convert\n",
        "from torch.ao.quantization.qconfig import QConfig\n",
        "from torch.ao.quantization.observer import MinMaxObserver\n",
        "from torch.ao.quantization.fake_quantize import FakeQuantize"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-03-22T14:07:29.260301Z",
          "iopub.execute_input": "2025-03-22T14:07:29.260648Z",
          "iopub.status.idle": "2025-03-22T14:07:29.265298Z",
          "shell.execute_reply.started": "2025-03-22T14:07:29.260608Z",
          "shell.execute_reply": "2025-03-22T14:07:29.264404Z"
        },
        "papermill": {
          "duration": 0.503668,
          "end_time": "2025-03-20T08:25:12.730266",
          "exception": false,
          "start_time": "2025-03-20T08:25:12.226598",
          "status": "completed"
        },
        "tags": [],
        "trusted": true,
        "id": "a46153d7"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "class QuantizedVGG19(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(QuantizedVGG19, self).__init__()\n",
        "\n",
        "        vgg19 = get_model(num_classes)\n",
        "\n",
        "        # Get features and classifier\n",
        "        self.features = vgg19.features\n",
        "        self.avgpool = vgg19.avgpool\n",
        "        self.classifier = vgg19.classifier\n",
        "\n",
        "        # Modify last layer\n",
        "        self.classifier[6] = nn.Linear(4096, num_classes)\n",
        "\n",
        "        # Quantization stubs\n",
        "        self.quant = QuantStub()\n",
        "        self.dequant = DeQuantStub()\n",
        "\n",
        "        # Initialize the new classifier layer\n",
        "        nn.init.kaiming_normal_(self.classifier[6].weight)\n",
        "        nn.init.constant_(self.classifier[6].bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Quantize input\n",
        "        x = self.quant(x)\n",
        "\n",
        "        # Forward pass\n",
        "        x = self.features(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.classifier(x)\n",
        "\n",
        "        # Dequantize output\n",
        "        x = self.dequant(x)\n",
        "        return x\n",
        "\n",
        "    def fuse_model(self):\n",
        "        \"\"\"\n",
        "        Fuse Conv+ReLU layers in VGG19.features. Skip invalid pairs like ReLU+MaxPool.\n",
        "        \"\"\"\n",
        "        fuse_list = []\n",
        "        for i in range(len(self.features) - 1):\n",
        "            if isinstance(self.features[i], nn.Conv2d) and isinstance(self.features[i + 1], nn.ReLU):\n",
        "                fuse_list.append([str(i), str(i + 1)])\n",
        "\n",
        "        torch.quantization.fuse_modules(self.features, fuse_list, inplace=True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-03-22T14:07:29.266305Z",
          "iopub.execute_input": "2025-03-22T14:07:29.266808Z",
          "iopub.status.idle": "2025-03-22T14:07:29.285535Z",
          "shell.execute_reply.started": "2025-03-22T14:07:29.266785Z",
          "shell.execute_reply": "2025-03-22T14:07:29.284892Z"
        },
        "papermill": {
          "duration": 0.509587,
          "end_time": "2025-03-20T08:25:13.829487",
          "exception": false,
          "start_time": "2025-03-20T08:25:13.3199",
          "status": "completed"
        },
        "tags": [],
        "trusted": true,
        "id": "033f0708"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8 bit Quantization"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.539838,
          "end_time": "2025-03-20T08:25:14.859775",
          "exception": false,
          "start_time": "2025-03-20T08:25:14.319937",
          "status": "completed"
        },
        "tags": [],
        "id": "47c8ba5a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_model_for_quantization(model, device='cpu', bit=8):\n",
        "    model.train()\n",
        "\n",
        "    if device == 'cpu':\n",
        "        backend = 'fbgemm'  # for x86 CPU\n",
        "    else:\n",
        "        backend = 'qnnpack'  # for ARM CPU\n",
        "\n",
        "    if bit == 4:\n",
        "        model.qconfig = QConfig(\n",
        "            activation=FakeQuantize.with_args(\n",
        "                observer=MinMaxObserver,\n",
        "                quant_min=0, quant_max=15,  # 4-bit range\n",
        "                dtype=torch.quint8,\n",
        "                qscheme=torch.per_tensor_affine\n",
        "            ),\n",
        "            weight=FakeQuantize.with_args(\n",
        "                observer=MinMaxObserver,\n",
        "                quant_min=-8, quant_max=7,  # 4-bit signed range (-8 to 7)\n",
        "                dtype=torch.qint8,\n",
        "                qscheme=torch.per_tensor_affine\n",
        "            )\n",
        "        )\n",
        "    if bit == 8:\n",
        "        model.qconfig = torch.quantization.get_default_qat_qconfig(backend)\n",
        "\n",
        "    model.fuse_model()\n",
        "\n",
        "    torch.quantization.prepare_qat(model, inplace=True)\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-03-22T14:07:29.286169Z",
          "iopub.execute_input": "2025-03-22T14:07:29.286383Z",
          "iopub.status.idle": "2025-03-22T14:07:29.299509Z",
          "shell.execute_reply.started": "2025-03-22T14:07:29.286363Z",
          "shell.execute_reply": "2025-03-22T14:07:29.29885Z"
        },
        "papermill": {
          "duration": 0.546067,
          "end_time": "2025-03-20T08:25:15.89343",
          "exception": false,
          "start_time": "2025-03-20T08:25:15.347363",
          "status": "completed"
        },
        "tags": [],
        "trusted": true,
        "id": "72421b5a"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def run_8bit_quantization(num_epochs, batch_size, num_samples):\n",
        "\n",
        "    model = QuantizedVGG19(num_classes=10)\n",
        "    original_size = get_model_file_size(model)\n",
        "    model = prepare_model_for_quantization(model, device='cpu', bit=8)\n",
        "\n",
        "    print(\"\\nModel Summary:\")\n",
        "    print(summary(model, input_size=(batch_size, 3, 224, 224)))\n",
        "    flops = count_flops(model)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=0.001,  momentum=0.9)\n",
        "\n",
        "    train_losses, train_accs = [], []\n",
        "    val_losses, val_accs = [], []\n",
        "\n",
        "    early_stopping = EarlyStopping(patience=5, min_delta=0.001)\n",
        "\n",
        "    print(\"\\nStarting training with 8 bit quantized model...\")\n",
        "    epochs_performed = 0\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
        "        # Training phase\n",
        "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "        train_losses.append(train_loss)\n",
        "        train_accs.append(train_acc)\n",
        "\n",
        "        # Validation phase\n",
        "        eval_metrics = evaluate(model, val_loader, criterion, device)\n",
        "        val_losses.append(eval_metrics['loss'])\n",
        "        val_accs.append(eval_metrics['acc'])\n",
        "        epochs_performed = epochs_performed + 1\n",
        "\n",
        "        # Early stopping logic\n",
        "        early_stopping(eval_metrics['loss'], model)\n",
        "        if early_stopping.early_stop:\n",
        "            print(\"Early stopping triggered\")\n",
        "            break\n",
        "\n",
        "    plot_metrics(train_losses, train_accs, val_losses, val_accs)\n",
        "    model = early_stopping.best_model\n",
        "    print(\"\\nConverting to quantized model...\")\n",
        "    quantized_model = torch.quantization.convert(model.eval().cpu(), inplace=False)\n",
        "    scripted_model = torch.jit.script(quantized_model)\n",
        "    scripted_model.save(\"qat_8bit_cifar10.pt\")\n",
        "\n",
        "    quantized_size = get_model_file_size(quantized_model)\n",
        "\n",
        "    cpu_metrics = evaluate(quantized_model, val_loader, criterion, 'cpu')\n",
        "\n",
        "    size_reduction = f\"{(original_size - quantized_size) / original_size * 100:.2f}%\"\n",
        "\n",
        "    result = print_and_get_metrics(\n",
        "        \"Quantization 8bit - CIFAR-10\",\n",
        "        epochs_performed, cpu_metrics, quantized_size, flops,\n",
        "        size_reduction=size_reduction\n",
        "    )\n",
        "\n",
        "    return quantized_model, result"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-03-22T14:07:29.300229Z",
          "iopub.execute_input": "2025-03-22T14:07:29.300416Z",
          "iopub.status.idle": "2025-03-22T14:07:29.317674Z",
          "shell.execute_reply.started": "2025-03-22T14:07:29.300399Z",
          "shell.execute_reply": "2025-03-22T14:07:29.316966Z"
        },
        "papermill": {
          "duration": 0.507383,
          "end_time": "2025-03-20T08:25:16.888271",
          "exception": false,
          "start_time": "2025-03-20T08:25:16.380888",
          "status": "completed"
        },
        "tags": [],
        "trusted": true,
        "id": "07facc10"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "qat_8bit_cifar10, qat_8bit_result = run_8bit_quantization(num_epochs, batch_size, num_samples)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-03-22T14:07:29.318521Z",
          "iopub.execute_input": "2025-03-22T14:07:29.31882Z",
          "iopub.status.idle": "2025-03-22T14:14:29.213204Z",
          "shell.execute_reply.started": "2025-03-22T14:07:29.318789Z",
          "shell.execute_reply": "2025-03-22T14:14:29.212174Z"
        },
        "papermill": {
          "duration": 1398.326628,
          "end_time": "2025-03-20T08:48:35.754515",
          "exception": false,
          "start_time": "2025-03-20T08:25:17.427887",
          "status": "completed"
        },
        "tags": [],
        "trusted": true,
        "id": "14f4b8c7"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4bit Quantization"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.727589,
          "end_time": "2025-03-20T08:48:37.210599",
          "exception": false,
          "start_time": "2025-03-20T08:48:36.48301",
          "status": "completed"
        },
        "tags": [],
        "id": "c2edd43d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "from torch.quantization import QuantStub, DeQuantStub, prepare_qat, convert"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-03-22T14:14:29.214929Z",
          "iopub.execute_input": "2025-03-22T14:14:29.215302Z",
          "iopub.status.idle": "2025-03-22T14:14:29.219405Z",
          "shell.execute_reply.started": "2025-03-22T14:14:29.215262Z",
          "shell.execute_reply": "2025-03-22T14:14:29.218638Z"
        },
        "papermill": {
          "duration": 0.740308,
          "end_time": "2025-03-20T08:48:38.617147",
          "exception": false,
          "start_time": "2025-03-20T08:48:37.876839",
          "status": "completed"
        },
        "tags": [],
        "trusted": true,
        "id": "7a34ef76"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def run_4bit_quantization(num_epochs, batch_size, num_samples):\n",
        "    model = QuantizedVGG19(num_classes=10).to(device)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9,weight_decay=5e-4)\n",
        "\n",
        "    for epoch in range(5):\n",
        "        print(f\"[Warm-up Epoch {epoch + 1}/5]\")\n",
        "        train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "        eval_metrics = evaluate(model, val_loader, criterion, device)\n",
        "        print(f\"Validation Accuracy: {eval_metrics['acc']:.4f}\")\n",
        "\n",
        "    original_size = get_model_file_size(model)\n",
        "    model = prepare_model_for_quantization(model, device, bit=4)\n",
        "    model.apply(torch.quantization.disable_observer)\n",
        "\n",
        "    print(\"\\nModel Summary:\")\n",
        "    print(summary(model, input_size=(batch_size, 3, 224, 224)))\n",
        "    flops = count_flops(model)\n",
        "\n",
        "    train_losses, train_accs = [], []\n",
        "    val_losses, val_accs = [], []\n",
        "    best_acc = 0.0\n",
        "\n",
        "    early_stopping = EarlyStopping(patience=5, min_delta=0.001)\n",
        "\n",
        "    print(\"\\nStarting training with 4bit quantized model...\")\n",
        "    epochs_performed = 0\n",
        "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
        "\n",
        "        if epoch == 3:\n",
        "            model.apply(torch.quantization.enable_observer)\n",
        "\n",
        "        # Training phase\n",
        "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "        train_losses.append(train_loss)\n",
        "        train_accs.append(train_acc)\n",
        "\n",
        "        # Validation phase\n",
        "        eval_metrics = evaluate(model, val_loader, criterion, device)\n",
        "        val_losses.append(eval_metrics['loss'])\n",
        "        val_accs.append(eval_metrics['acc'])\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        epochs_performed = epochs_performed + 1\n",
        "\n",
        "        if eval_metrics['acc'] > best_acc:\n",
        "            best_acc = eval_metrics['acc']\n",
        "            print(f\"New best model saved with accuracy: {best_acc:.4f}\")\n",
        "\n",
        "        # Early stopping logic\n",
        "        early_stopping(eval_metrics['loss'], model)\n",
        "        if early_stopping.early_stop:\n",
        "            print(\"Early stopping triggered\")\n",
        "            break\n",
        "\n",
        "    plot_metrics(train_losses, train_accs, val_losses, val_accs)\n",
        "\n",
        "    model = early_stopping.best_model\n",
        "    quantized_model = torch.quantization.convert(model.eval().cpu(), inplace=False)\n",
        "    scripted_model = torch.jit.script(quantized_model)\n",
        "    scripted_model.save(\"qat_4bit_cifar10.pt\")\n",
        "\n",
        "    quantized_size = get_model_file_size(quantized_model)\n",
        "\n",
        "    cpu_metrics = evaluate(quantized_model, val_loader, criterion, 'cpu')\n",
        "\n",
        "    size_reduction = f\"{(original_size - quantized_size) / original_size * 100:.2f}%\"\n",
        "\n",
        "    result = print_and_get_metrics(\n",
        "        \"Quantization 4bit - CIFAR-10\",\n",
        "        epochs_performed, cpu_metrics, quantized_size, flops,\n",
        "        size_reduction=size_reduction\n",
        "    )\n",
        "\n",
        "    return quantized_model, result"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-03-22T14:14:29.220204Z",
          "iopub.execute_input": "2025-03-22T14:14:29.220386Z",
          "iopub.status.idle": "2025-03-22T14:14:29.241028Z",
          "shell.execute_reply.started": "2025-03-22T14:14:29.220369Z",
          "shell.execute_reply": "2025-03-22T14:14:29.240237Z"
        },
        "papermill": {
          "duration": 0.745078,
          "end_time": "2025-03-20T08:48:41.438128",
          "exception": false,
          "start_time": "2025-03-20T08:48:40.69305",
          "status": "completed"
        },
        "tags": [],
        "trusted": true,
        "id": "23a0a9aa"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "qat_4bit_cifar10, qat_4bit_result = run_4bit_quantization(num_epochs, batch_size, num_samples)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-03-22T14:14:29.241944Z",
          "iopub.execute_input": "2025-03-22T14:14:29.242225Z",
          "iopub.status.idle": "2025-03-22T14:25:30.684214Z",
          "shell.execute_reply.started": "2025-03-22T14:14:29.242193Z",
          "shell.execute_reply": "2025-03-22T14:25:30.682989Z"
        },
        "papermill": {
          "duration": 1093.562783,
          "end_time": "2025-03-20T09:06:55.671543",
          "exception": false,
          "start_time": "2025-03-20T08:48:42.10876",
          "status": "completed"
        },
        "tags": [],
        "trusted": true,
        "id": "aab17eee"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def get_quantized_weights_mapping(model):\n",
        "    quantized_weights = {}\n",
        "    import torch.nn.quantized as nnq\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, (nnq.Conv2d, nnq.Linear)):\n",
        "            # Use dequantize() to compare as floating-point values.\n",
        "            quantized_weights[name] = module.weight().dequantize()\n",
        "    return quantized_weights\n",
        "\n",
        "def find_max_changed_layer_mapping(baseline_model, quantized_weights_map):\n",
        "    max_composite = -np.inf\n",
        "    max_layer = None\n",
        "    for name, param in baseline_model.named_parameters():\n",
        "        if \"weight\" not in name:\n",
        "            continue\n",
        "\n",
        "        # Adjust the baseline name to match the keys in quantized_weights_map by stripping '.weight'\n",
        "        quant_name = name[:-7] if name.endswith(\".weight\") else name\n",
        "\n",
        "        if quant_name not in quantized_weights_map:\n",
        "            print(f\"Layer {name} not found in quantized weights mapping\")\n",
        "            continue\n",
        "\n",
        "        baseline_weights = param.detach().cpu().numpy().flatten()\n",
        "        quantized_weights = quantized_weights_map[quant_name].detach().cpu().numpy().flatten()\n",
        "\n",
        "        l2_baseline = np.linalg.norm(baseline_weights)\n",
        "        l2_quantized = np.linalg.norm(quantized_weights)\n",
        "        l1_baseline = np.sum(np.abs(baseline_weights))\n",
        "        l1_quantized = np.sum(np.abs(quantized_weights))\n",
        "\n",
        "        # Skip layers with zero norm to avoid division by zero.\n",
        "        if l2_baseline == 0 or l1_baseline == 0:\n",
        "            continue\n",
        "\n",
        "        l2_change = np.abs(l2_baseline - l2_quantized) / l2_baseline\n",
        "        l1_change = np.abs(l1_baseline - l1_quantized) / l1_baseline\n",
        "\n",
        "        composite = (l2_change + l1_change) / 2.0\n",
        "\n",
        "        improvement_threshold = 0.1\n",
        "        improved = \"Yes\" if composite < improvement_threshold else \"No\"\n",
        "\n",
        "        l2_improvement = (1 - l2_change) * 100\n",
        "        l1_improvement = (1 - l1_change) * 100\n",
        "        composite_improvement = (1 - composite) * 100\n",
        "\n",
        "        if composite > max_composite:\n",
        "            max_composite = composite\n",
        "            max_layer = name\n",
        "    return max_layer, max_composite\n",
        "\n",
        "def comparison_diagram_extended_quantization_mapping(baseline_model, quantized_weights_map, layer_name, quantization_bits):\n",
        "    # Get baseline weights from baseline model.\n",
        "    baseline_weights = dict(baseline_model.named_parameters())[layer_name].detach().cpu().numpy().flatten()\n",
        "    # Adjust layer name for quantized mapping (strip \".weight\")\n",
        "    quant_layer_name = layer_name\n",
        "    if quant_layer_name.endswith(\".weight\"):\n",
        "        quant_layer_name = quant_layer_name[:-7]\n",
        "    quantized_weights = quantized_weights_map[quant_layer_name].detach().cpu().numpy().flatten()\n",
        "\n",
        "    # Compute norms.\n",
        "    l2_baseline = np.linalg.norm(baseline_weights)\n",
        "    l2_quantized = np.linalg.norm(quantized_weights)\n",
        "    l1_baseline = np.sum(np.abs(baseline_weights))\n",
        "    l1_quantized = np.sum(np.abs(quantized_weights))\n",
        "\n",
        "    # Compute quantization error.\n",
        "    quant_error = baseline_weights - quantized_weights\n",
        "\n",
        "    # Create a 1x5 figure.\n",
        "    fig, axs = plt.subplots(1, 5, figsize=(30, 5))\n",
        "\n",
        "    # Panel 1: Baseline Weight Distribution.\n",
        "    axs[0].hist(baseline_weights, bins=30, alpha=0.7, color='blue')\n",
        "    axs[0].set_title('Baseline Weight Distribution')\n",
        "    axs[0].set_xlabel('Weight values')\n",
        "    axs[0].set_ylabel('Frequency')\n",
        "\n",
        "    # Panel 2: Quantized Weight Distribution.\n",
        "    axs[1].hist(quantized_weights, bins=30, alpha=0.7, color='orange')\n",
        "    axs[1].set_title(f'{quantization_bits}-bit Quantized Distribution')\n",
        "    axs[1].set_xlabel('Weight values')\n",
        "    axs[1].set_ylabel('Frequency')\n",
        "\n",
        "    # Panel 3: L2 Norm Comparison.\n",
        "    axs[2].bar(['Baseline', f'{quantization_bits}-bit'], [l2_baseline, l2_quantized], color=['blue', 'orange'])\n",
        "    axs[2].set_title('L2 Norm')\n",
        "    axs[2].set_ylabel('L2 Norm')\n",
        "\n",
        "    # Panel 4: L1 Norm Comparison.\n",
        "    axs[3].bar(['Baseline', f'{quantization_bits}-bit'], [l1_baseline, l1_quantized], color=['blue', 'orange'])\n",
        "    axs[3].set_title('L1 Norm')\n",
        "    axs[3].set_ylabel('L1 Norm')\n",
        "\n",
        "    # Panel 5: Quantization Error Distribution.\n",
        "    axs[4].hist(quant_error, bins=30, alpha=0.7, color='green')\n",
        "    axs[4].set_title('Quantization Error Distribution')\n",
        "    axs[4].set_xlabel('Error value')\n",
        "    axs[4].set_ylabel('Frequency')\n",
        "\n",
        "    plt.suptitle(f'Layer {layer_name} Quantization Visualization: {quantization_bits}-bit', fontsize=16)\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
        "    plt.show()\n",
        "\n",
        "def get_overall_improvement(baseline_model, quantized_weights_map):\n",
        "    total_layers = 0\n",
        "    sum_l2_improvement = 0\n",
        "    sum_l1_improvement = 0\n",
        "    sum_composite_improvement = 0\n",
        "\n",
        "    for name, param in baseline_model.named_parameters():\n",
        "        if \"weight\" not in name:\n",
        "            continue\n",
        "\n",
        "        quant_name = name[:-7] if name.endswith(\".weight\") else name\n",
        "\n",
        "        if quant_name not in quantized_weights_map:\n",
        "            print(f\"Layer {name} not found in quantized weights mapping\")\n",
        "            continue\n",
        "\n",
        "        baseline_weights = param.detach().cpu().numpy().flatten()\n",
        "        quantized_weights = quantized_weights_map[quant_name].detach().cpu().numpy().flatten()\n",
        "\n",
        "        l2_baseline = np.linalg.norm(baseline_weights)\n",
        "        l2_quantized = np.linalg.norm(quantized_weights)\n",
        "        l1_baseline = np.sum(np.abs(baseline_weights))\n",
        "        l1_quantized = np.sum(np.abs(quantized_weights))\n",
        "\n",
        "        if l2_baseline == 0 or l1_baseline == 0:\n",
        "            continue\n",
        "\n",
        "        l2_change = np.abs(l2_baseline - l2_quantized) / l2_baseline\n",
        "        l1_change = np.abs(l1_baseline - l1_quantized) / l1_baseline\n",
        "        composite_change = (l2_change + l1_change) / 2.0\n",
        "\n",
        "        l2_improvement = (1 - l2_change) * 100\n",
        "        l1_improvement = (1 - l1_change) * 100\n",
        "        composite_improvement = (1 - composite_change) * 100\n",
        "\n",
        "        total_layers += 1\n",
        "        sum_l2_improvement += l2_improvement\n",
        "        sum_l1_improvement += l1_improvement\n",
        "        sum_composite_improvement += composite_improvement\n",
        "\n",
        "    if total_layers > 0:\n",
        "        avg_l2_improvement = sum_l2_improvement / total_layers\n",
        "        avg_l1_improvement = sum_l1_improvement / total_layers\n",
        "        avg_composite_improvement = sum_composite_improvement / total_layers\n",
        "    else:\n",
        "        avg_l2_improvement, avg_l1_improvement, avg_composite_improvement = 0, 0, 0\n",
        "\n",
        "    print(\"Overall Improvement Metrics:\")\n",
        "    print(f\"  Average L2 Improvement: {avg_l2_improvement:.2f}%\")\n",
        "    print(f\"  Average L1 Improvement: {avg_l1_improvement:.2f}%\")\n",
        "    print(f\"  Average Composite Improvement: {avg_composite_improvement:.2f}%\")\n",
        "\n",
        "    return avg_l2_improvement, avg_l1_improvement, avg_composite_improvement\n",
        "\n",
        "def comparison_diagram_max_changed_layer_mapping(baseline_model, quantized_weights_map, quantization_bits):\n",
        "    # Compute and print overall improvements\n",
        "    avg_l2_improvement, avg_l1_improvement, avg_composite_improvement = get_overall_improvement(baseline_model, quantized_weights_map)\n",
        "\n",
        "    print(\"\\nOverall Improvement Metrics:\")\n",
        "    print(f\"  Average L2 Improvement: {avg_l2_improvement:.2f}%\")\n",
        "    print(f\"  Average L1 Improvement: {avg_l1_improvement:.2f}%\")\n",
        "    print(f\"  Average Composite Improvement: {avg_composite_improvement:.2f}%\")\n",
        "\n",
        "    max_layer, composite = find_max_changed_layer_mapping(baseline_model, quantized_weights_map)\n",
        "    if max_layer is None:\n",
        "        print(\"No layer found with significant change.\")\n",
        "    else:\n",
        "        print(f\"Layer with maximum change: {max_layer}\\nComposite Change: {composite:.3f}\")\n",
        "        comparison_diagram_extended_quantization_mapping(baseline_model, quantized_weights_map, max_layer, quantization_bits)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-03-22T14:25:30.686059Z",
          "iopub.execute_input": "2025-03-22T14:25:30.686315Z",
          "iopub.status.idle": "2025-03-22T14:25:30.704422Z",
          "shell.execute_reply.started": "2025-03-22T14:25:30.686291Z",
          "shell.execute_reply": "2025-03-22T14:25:30.703465Z"
        },
        "papermill": {
          "duration": 0.893866,
          "end_time": "2025-03-20T09:06:57.435837",
          "exception": false,
          "start_time": "2025-03-20T09:06:56.541971",
          "status": "completed"
        },
        "tags": [],
        "trusted": true,
        "id": "900e9ef9"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "quantized_weights_map = get_quantized_weights_mapping(qat_8bit_cifar10)\n",
        "comparison_diagram_max_changed_layer_mapping(baseline_cifar10, quantized_weights_map, quantization_bits=8)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-03-22T14:25:30.705437Z",
          "iopub.execute_input": "2025-03-22T14:25:30.705767Z",
          "iopub.status.idle": "2025-03-22T14:25:35.793519Z",
          "shell.execute_reply.started": "2025-03-22T14:25:30.705736Z",
          "shell.execute_reply": "2025-03-22T14:25:35.792453Z"
        },
        "papermill": {
          "duration": 5.533054,
          "end_time": "2025-03-20T09:07:03.778245",
          "exception": false,
          "start_time": "2025-03-20T09:06:58.245191",
          "status": "completed"
        },
        "tags": [],
        "trusted": true,
        "id": "2eb24d81"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "quantized_weights_map = get_quantized_weights_mapping(qat_4bit_cifar10)\n",
        "comparison_diagram_max_changed_layer_mapping(baseline_cifar10, quantized_weights_map, quantization_bits=4)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-03-22T14:25:35.794638Z",
          "iopub.execute_input": "2025-03-22T14:25:35.7949Z",
          "iopub.status.idle": "2025-03-22T14:25:40.66153Z",
          "shell.execute_reply.started": "2025-03-22T14:25:35.794876Z",
          "shell.execute_reply": "2025-03-22T14:25:40.66055Z"
        },
        "papermill": {
          "duration": 5.516613,
          "end_time": "2025-03-20T09:07:10.17533",
          "exception": false,
          "start_time": "2025-03-20T09:07:04.658717",
          "status": "completed"
        },
        "tags": [],
        "trusted": true,
        "id": "4ee11476"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def flatten_dict(d, parent_key='', sep='_'):\n",
        "    items = []\n",
        "    for k, v in d.items():\n",
        "        new_key = f\"{parent_key}{sep}{k}\" if parent_key else k\n",
        "        if isinstance(v, dict):\n",
        "            items.extend(flatten_dict(v, new_key, sep=sep).items())\n",
        "        else:\n",
        "            items.append((new_key, v))\n",
        "    return dict(items)\n",
        "\n",
        "# Flatten each dictionary in the list\n",
        "flat_data = [flatten_dict(item) for item in [baseline_result,\n",
        "                                             sp_result, usp_result,\n",
        "                                             qat_8bit_result, qat_4bit_result]]\n",
        "\n",
        "# Create a DataFrame from the flattened data\n",
        "df = pd.DataFrame(flat_data)\n",
        "\n",
        "\n",
        "# print(df)\n",
        "df"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-03-22T14:25:40.662742Z",
          "iopub.execute_input": "2025-03-22T14:25:40.663054Z",
          "iopub.status.idle": "2025-03-22T14:25:41.272879Z",
          "shell.execute_reply.started": "2025-03-22T14:25:40.663024Z",
          "shell.execute_reply": "2025-03-22T14:25:41.271955Z"
        },
        "papermill": {
          "duration": 1.93901,
          "end_time": "2025-03-20T09:07:12.992261",
          "exception": false,
          "start_time": "2025-03-20T09:07:11.053251",
          "status": "completed"
        },
        "tags": [],
        "trusted": true,
        "id": "56c68430"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}